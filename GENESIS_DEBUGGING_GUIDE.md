# Genesis AI-Powered Software Generator - Debugging Guide

## ğŸš¨ Current Status: Optimization Complete, AI Integration Pending

**Last Updated:** July 23, 2025  
**Status:** All services running, CrewAI-LLM compatibility issue identified

---

## ğŸ“‹ System Overview

Genesis is a multi-service AI-powered software generator with:
- **Frontend**: Tauri + React + TypeScript (Port 5173)
- **Backend**: Rust Actix-web API (Port 8080)  
- **AI Core**: Python FastAPI + CrewAI + Ollama (Port 8000)

---

## ğŸ”§ Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚    Backend      â”‚    â”‚    AI Core      â”‚
â”‚   (Tauri)       â”‚â—„â”€â”€â–ºâ”‚   (Rust)        â”‚â—„â”€â”€â–ºâ”‚   (Python)      â”‚
â”‚   Port 5173     â”‚    â”‚   Port 8080     â”‚    â”‚   Port 8000     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Health        â”‚    â”‚   Ollama        â”‚
                       â”‚   Monitoring    â”‚    â”‚   Models        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Working Components

### 1. **Frontend (Tauri + React)**
- âœ… Running on port 5173
- âœ… Connected to backend
- âœ… Modern UI with Chakra UI
- âœ… Real-time status updates
- âœ… File tree display ready

### 2. **Backend (Rust Actix-web)**
- âœ… Running on port 8080
- âœ… Health endpoint: `GET /health`
- âœ… Generate endpoint: `POST /generate`
- âœ… Project status endpoint: `GET /projects/{id}`
- âœ… Comprehensive error handling
- âœ… Request validation and rate limiting

### 3. **AI Core (Python FastAPI)**
- âœ… Running on port 8000
- âœ… Health endpoint: `GET /health`
- âœ… Run endpoint: `POST /run`
- âœ… Ollama connectivity verified
- âœ… Models available: `qwen2.5-coder:1.5b-base`, `llama3.1:8b`

### 4. **Ollama Integration**
- âœ… Ollama service running
- âœ… Models downloaded and available
- âœ… Direct LLM calls working
- âœ… Health checks passing

---

## âš ï¸ Known Issues

### **Primary Issue: CrewAI-LLM Compatibility**

**Problem:** LLM works directly but fails in CrewAI context
```
ERROR: LLM Failed
An unknown error occurred. Please check the details below.
```

**Root Cause:** Compatibility issue between `langchain_ollama.OllamaLLM` and CrewAI framework

**Evidence:**
- âœ… Direct LLM calls: `llm.invoke("Hello")` works
- âœ… Ollama API: Direct API calls work
- âŒ CrewAI integration: Fails with "LLM Failed"

**Attempted Fixes:**
1. âœ… Removed tools validation errors
2. âœ… Fixed agent-task assignments
3. âœ… Disabled memory to avoid ChromaDB dependency
4. âœ… Adjusted LLM configuration (temperature, verbose)
5. âœ… Tried different model priorities

---

## ğŸ› ï¸ Debugging Commands

### **Service Health Checks**

```bash
# Check all services
curl http://localhost:5173  # Frontend
curl http://localhost:8080/health  # Backend
curl http://localhost:8000/health  # AI Core
curl http://localhost:11434/api/tags  # Ollama
```

### **Test Generation Flow**

```bash
# Test backend generation
curl -X POST http://localhost:8080/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Create a simple React counter","backend":"ollama"}'

# Test AI core directly
curl -X POST http://localhost:8000/run \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Create a simple React counter","backend":"ollama"}'
```

### **LLM Testing**

```bash
# Test LLM directly
cd ai_core
python test_ollama.py

# Test CrewAI integration
python test_llm_crewai.py

# Debug specific components
python debug_test.py
```

---

## ğŸ“ Key Files and Their Purposes

### **Configuration Files**
- `ai_core/requirements.txt` - Python dependencies
- `backend/Cargo.toml` - Rust dependencies
- `genesis-frontend/package.json` - Node.js dependencies
- `env.example` - Environment configuration template

### **Core Implementation**
- `ai_core/main.py` - FastAPI server with CrewAI integration
- `ai_core/llm.py` - LLM configuration and initialization
- `ai_core/agents.py` - CrewAI agent definitions
- `ai_core/tasks.py` - CrewAI task definitions
- `backend/src/main.rs` - Rust API server
- `genesis-frontend/src/App.tsx` - React frontend

### **Error Handling & Monitoring**
- `ai_core/error_handling.py` - Custom exceptions and handlers
- `ai_core/logging_config.py` - Enhanced logging setup
- `backend/src/error.rs` - Rust error types
- `backend/src/health.rs` - Health check endpoints
- `scripts/monitor.py` - Service monitoring script

### **Testing & Debugging**
- `ai_core/test_ollama.py` - Ollama connectivity tests
- `ai_core/debug_test.py` - Component debugging
- `ai_core/test_llm_crewai.py` - CrewAI integration tests
- `tests/test_integration.py` - End-to-end integration tests

---

## ğŸ” Debugging Workflow

### **1. Service Startup Issues**
```bash
# Check if ports are in use
netstat -ano | findstr :8000
netstat -ano | findstr :8080
netstat -ano | findstr :5173

# Kill processes if needed
taskkill /F /PID <PID>
```

### **2. LLM Issues**
```bash
# Check Ollama status
ollama list
ollama serve

# Test specific model
ollama run qwen2.5-coder:1.5b-base "Hello"
```

### **3. CrewAI Issues**
```bash
# Test components individually
cd ai_core
python -c "from llm import get_llm; llm = get_llm(); print(llm.invoke('Hello'))"
python -c "from agents import create_agents; from llm import get_llm; agents = create_agents(get_llm()); print(len(agents))"
```

### **4. Dependency Issues**
```bash
# Python dependencies
pip install -r requirements.txt
pip list | grep crewai

# Rust dependencies
cd backend
cargo check
cargo build
```

---

## ğŸš€ Next Steps to Resolve CrewAI Issue

### **Option 1: Try Different LLM Wrapper**
```python
# In ai_core/llm.py, try:
from crewai import LLM

llm = LLM(
    provider="ollama",
    model=model_name,
    base_url="http://localhost:11434"
)
```

### **Option 2: Use Different CrewAI Version**
```bash
pip uninstall crewai
pip install crewai==0.28.0  # Try older version
```

### **Option 3: Custom LLM Adapter**
Create a custom adapter that wraps OllamaLLM for CrewAI compatibility.

### **Option 4: Alternative AI Framework**
Consider using LangChain directly or other AI frameworks.

---

## ğŸ“Š Performance Metrics

### **Current Response Times**
- Backend health check: ~50ms
- AI Core health check: ~200ms
- Ollama model generation: ~2-5s
- CrewAI execution: âŒ Failing

### **Resource Usage**
- Frontend: ~50MB RAM
- Backend: ~20MB RAM
- AI Core: ~100MB RAM
- Ollama: ~2-4GB RAM (depending on model)

---

## ğŸ”§ Environment Variables

### **Required Environment Variables**
```bash
# AI Core
OLLAMA_BASE_URL=http://localhost:11434
AI_CORE_HOST=127.0.0.1
AI_CORE_PORT=8000

# Backend
BACKEND_HOST=127.0.0.1
BACKEND_PORT=8080
AI_CORE_URL=http://127.0.0.1:8000

# Frontend
VITE_BACKEND_URL=http://127.0.0.1:8080
```

---

## ğŸ“ Support Information

### **Log Locations**
- AI Core logs: Console output + `ai_core/logs/`
- Backend logs: Console output
- Frontend logs: Browser console

### **Common Error Messages**
- `LLM Failed`: CrewAI-LLM compatibility issue
- `Connection refused`: Service not running
- `Model not found`: Ollama model not downloaded
- `Validation error`: Request format issue

### **Recovery Procedures**
1. Restart all services in order: Ollama â†’ AI Core â†’ Backend â†’ Frontend
2. Check port availability
3. Verify Ollama models are downloaded
4. Test individual components
5. Check environment variables

---

## ğŸ¯ Success Criteria

The system will be fully functional when:
- âœ… All services are running (COMPLETED)
- âœ… Health checks pass (COMPLETED)
- âœ… LLM works directly (COMPLETED)
- âœ… CrewAI executes successfully (PENDING)
- âœ… Project generation completes (PENDING)
- âœ… Frontend displays generated files (PENDING)

**Current Progress: 75% Complete** ğŸš€ 